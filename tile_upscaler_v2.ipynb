{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMylJLJOk0vaGAVs0yGYEkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/detektor777/colab_list_image/blob/main/tile_upscaler_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2KZndlYCoGY"
      },
      "outputs": [],
      "source": [
        "#@title ##**Install** { display-mode: \"form\" }\n",
        "%%capture\n",
        "!nvidia-smi\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ª–∏ —É–∂–µ\n",
        "if not os.path.exists('/content/installed_tile_upscaler'):\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üì¶ INSTALLING TILE UPSCALER\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º diffusers –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–∞–∫–µ—Ç—ã\n",
        "    print(\"\\n[1/4] Installing Python packages...\")\n",
        "    packages = [\n",
        "        'diffusers',\n",
        "        'transformers',\n",
        "        'accelerate',\n",
        "        'safetensors',\n",
        "        'invisible-watermark',\n",
        "        'huggingface_hub'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"  ‚Üí {package}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "    print(\"  ‚úì Python packages installed\")\n",
        "\n",
        "    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Real-ESRGAN\n",
        "    print(\"\\n[2/4] Installing Real-ESRGAN...\")\n",
        "\n",
        "    if not os.path.exists('/content/Real-ESRGAN'):\n",
        "        print(\"  ‚Üí Cloning repository...\")\n",
        "        get_ipython().system('git clone -q https://github.com/xinntao/Real-ESRGAN.git /content/Real-ESRGAN')\n",
        "\n",
        "    os.chdir('/content/Real-ESRGAN')\n",
        "\n",
        "    print(\"  ‚Üí Installing dependencies...\")\n",
        "    get_ipython().system('pip install -q basicsr facexlib gfpgan -r requirements.txt')\n",
        "    get_ipython().system('python setup.py develop > /dev/null 2>&1')\n",
        "\n",
        "    print(\"  ‚Üí Fixing compatibility issues...\")\n",
        "\n",
        "    # URL –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ GFPGAN\n",
        "    new_model_path = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth'\n",
        "\n",
        "    # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª inference_realesrgan.py –∏ –∏–∑–º–µ–Ω—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏\n",
        "    inference_file = '/content/Real-ESRGAN/inference_realesrgan.py'\n",
        "    if os.path.exists(inference_file):\n",
        "        with open(inference_file, 'r') as f:\n",
        "            script_content = f.read()\n",
        "\n",
        "        # –ò–∑–º–µ–Ω—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ —Å–∫—Ä–∏–ø—Ç–∞\n",
        "        new_script_content = re.sub(r\"(model_path\\s*=\\s*[\\\"\\']).*?([\\\"\\'])\", rf\"\\g<1>{new_model_path}\\g<2>\", script_content)\n",
        "\n",
        "        with open(inference_file, 'w') as f:\n",
        "            f.write(new_script_content)\n",
        "\n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä—Å–∏—é Python\n",
        "    python_version = f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "    degradations_path = f'/usr/local/lib/{python_version}/dist-packages/basicsr/data/degradations.py'\n",
        "\n",
        "    # –î–µ–ª–∞–µ–º –±—ç–∫–∞–ø\n",
        "    if os.path.exists(degradations_path):\n",
        "        get_ipython().system(f\"cp -n {degradations_path} {degradations_path}.backup\")\n",
        "\n",
        "    # –§–∏–∫—Å–∏–º –∏–º–ø–æ—Ä—Ç rgb_to_grayscale\n",
        "    get_ipython().system(f\"sed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' {degradations_path}\")\n",
        "\n",
        "    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–¥ degradations\n",
        "    degradations_code = '''import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def random_add_gaussian_noise_pt(img, sigma_range=(0, 1.0), gray_prob=0, noise_gray_prob=0, clip=True, rounds=False):\n",
        "    noise_sigma = random.uniform(*sigma_range)\n",
        "\n",
        "    if random.random() < gray_prob:\n",
        "        img = rgb_to_grayscale(img)\n",
        "\n",
        "    if random.random() < noise_gray_prob:\n",
        "        noise = torch.randn(*img.shape[1:], device=img.device) * noise_sigma\n",
        "        noise = noise.unsqueeze(0).repeat(img.shape[0], 1, 1)\n",
        "    else:\n",
        "        noise = torch.randn_like(img) * noise_sigma\n",
        "\n",
        "    out = img + noise\n",
        "\n",
        "    if clip and rounds:\n",
        "        out = torch.clamp((out * 255.0).round(), 0, 255) / 255.\n",
        "    elif clip:\n",
        "        out = torch.clamp(out, 0, 1)\n",
        "    elif rounds:\n",
        "        out = (out * 255.0).round() / 255.\n",
        "\n",
        "    return out\n",
        "\n",
        "def random_add_poisson_noise_pt(img, scale_range=(0, 1.0), gray_prob=0, clip=True, rounds=False):\n",
        "    scale = random.uniform(*scale_range)\n",
        "\n",
        "    if random.random() < gray_prob:\n",
        "        img = rgb_to_grayscale(img)\n",
        "\n",
        "    noise = torch.poisson(img * scale) / scale - img\n",
        "    out = img + noise\n",
        "\n",
        "    if clip and rounds:\n",
        "        out = torch.clamp((out * 255.0).round(), 0, 255) / 255.\n",
        "    elif clip:\n",
        "        out = torch.clamp(out, 0, 1)\n",
        "    elif rounds:\n",
        "        out = (out * 255.0).round() / 255.\n",
        "\n",
        "    return out\n",
        "\n",
        "def rgb_to_grayscale(img):\n",
        "    if img.shape[0] != 3:\n",
        "        raise ValueError('Input image must have 3 channels')\n",
        "    rgb_weights = torch.tensor([0.2989, 0.5870, 0.1140], device=img.device)\n",
        "    grayscale = torch.sum(img * rgb_weights.view(-1, 1, 1), dim=0, keepdim=True)\n",
        "    return grayscale\n",
        "\n",
        "def circular_lowpass_kernel(cutoff, kernel_size, pad_to=0):\n",
        "    if pad_to == 0:\n",
        "        pad_to = kernel_size\n",
        "    assert pad_to >= kernel_size, 'Pad size must be larger than kernel size'\n",
        "\n",
        "    def _scaled_sinc(x):\n",
        "        if x == 0:\n",
        "            return torch.tensor(1.)\n",
        "        x = x * math.pi\n",
        "        return torch.sin(x) / x\n",
        "\n",
        "    half_size = (kernel_size - 1) / 2.\n",
        "    grid = torch.linspace(-half_size, half_size, kernel_size)\n",
        "    x, y = torch.meshgrid(grid, grid)\n",
        "    dist = torch.sqrt(x**2 + y**2)\n",
        "\n",
        "    kernel = _scaled_sinc(dist * cutoff)\n",
        "    kernel = kernel / kernel.sum()\n",
        "\n",
        "    if pad_to > kernel_size:\n",
        "        pad = (pad_to - kernel_size) // 2\n",
        "        kernel = F.pad(kernel, [pad] * 4)\n",
        "\n",
        "    return kernel\n",
        "\n",
        "def random_mixed_kernels(\n",
        "        kernel_list,\n",
        "        kernel_prob,\n",
        "        kernel_size=21,\n",
        "        blur_sigma=0.1,\n",
        "        blur_sigma_min=0.1,\n",
        "        blur_sigma_max=10.0,\n",
        "        blur_kernel_size=21,\n",
        "        pad_to=0):\n",
        "    num_kernels = len(kernel_list)\n",
        "    kernel_type = np.random.choice(kernel_list, p=kernel_prob)\n",
        "\n",
        "    if pad_to == 0:\n",
        "        pad_to = kernel_size\n",
        "\n",
        "    if kernel_type == 'iso':\n",
        "        sigma = np.random.uniform(blur_sigma_min, blur_sigma_max)\n",
        "        kernel = _generate_isotropic_gaussian_kernel(kernel_size, sigma, pad_to)\n",
        "    elif kernel_type == 'aniso':\n",
        "        sigma_x = np.random.uniform(blur_sigma_min, blur_sigma_max)\n",
        "        sigma_y = np.random.uniform(blur_sigma_min, blur_sigma_max)\n",
        "        rotation = np.random.uniform(-np.pi, np.pi)\n",
        "        kernel = _generate_anisotropic_gaussian_kernel(kernel_size, sigma_x, sigma_y, rotation, pad_to)\n",
        "    else:  # general\n",
        "        kernel = circular_lowpass_kernel(blur_sigma, kernel_size, pad_to)\n",
        "\n",
        "    return kernel\n",
        "\n",
        "def _generate_isotropic_gaussian_kernel(kernel_size=21, sigma=0.1, pad_to=0):\n",
        "    if pad_to == 0:\n",
        "        pad_to = kernel_size\n",
        "\n",
        "    half_size = (kernel_size - 1) / 2.\n",
        "    grid = torch.linspace(-half_size, half_size, kernel_size)\n",
        "    x, y = torch.meshgrid(grid, grid)\n",
        "    kernel = torch.exp(-(x**2 + y**2) / (2 * sigma**2))\n",
        "    kernel = kernel / kernel.sum()\n",
        "\n",
        "    if pad_to > kernel_size:\n",
        "        pad = (pad_to - kernel_size) // 2\n",
        "        kernel = F.pad(kernel, [pad] * 4)\n",
        "\n",
        "    return kernel\n",
        "\n",
        "def _generate_anisotropic_gaussian_kernel(kernel_size=21, sigma_x=0.1, sigma_y=0.1, rotation=0, pad_to=0):\n",
        "    if pad_to == 0:\n",
        "        pad_to = kernel_size\n",
        "\n",
        "    half_size = (kernel_size - 1) / 2.\n",
        "    grid = torch.linspace(-half_size, half_size, kernel_size)\n",
        "    x, y = torch.meshgrid(grid, grid)\n",
        "\n",
        "    cos_theta = torch.cos(torch.tensor(rotation))\n",
        "    sin_theta = torch.sin(torch.tensor(rotation))\n",
        "    x_rot = cos_theta * x - sin_theta * y\n",
        "    y_rot = sin_theta * x + cos_theta * y\n",
        "\n",
        "    kernel = torch.exp(-(x_rot**2 / (2 * sigma_x**2) + y_rot**2 / (2 * sigma_y**2)))\n",
        "    kernel = kernel / kernel.sum()\n",
        "\n",
        "    if pad_to > kernel_size:\n",
        "        pad = (pad_to - kernel_size) // 2\n",
        "        kernel = F.pad(kernel, [pad] * 4)\n",
        "\n",
        "    return kernel\n",
        "'''\n",
        "\n",
        "    with open(degradations_path, 'w') as f:\n",
        "        f.write(degradations_code)\n",
        "\n",
        "    print(\"  ‚úì Real-ESRGAN installed\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –¥–ª—è Tile-Upscaler\n",
        "    print(\"\\n[3/4] Creating project structure...\")\n",
        "\n",
        "    if not os.path.exists('/content/Tile-Upscaler'):\n",
        "        get_ipython().system('git clone -q https://github.com/gokayfem/Tile-Upscaler.git /content/Tile-Upscaler')\n",
        "\n",
        "    os.chdir('/content/Tile-Upscaler')\n",
        "\n",
        "    for folder in ['models/ControlNet', 'models/models/Stable-diffusion', 'models/VAE',\n",
        "                   'models/embeddings', 'models/Lora', 'models/upscalers']:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "    print(\"  ‚úì Directories created\")\n",
        "\n",
        "    # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—è hf_hub_download\n",
        "    print(\"\\n[4/4] Downloading AI models from Hugging Face...\")\n",
        "    print(\"  (This will take 5-10 minutes)\")\n",
        "\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    models = {\n",
        "        \"Stable Diffusion Model\": {\n",
        "            \"repo_id\": \"dantea1118/juggernaut_reborn\",\n",
        "            \"filename\": \"juggernaut_reborn.safetensors\",\n",
        "            \"local_dir\": \"models/models/Stable-diffusion\"\n",
        "        },\n",
        "        \"RealESRGAN x2\": {\n",
        "            \"repo_id\": \"ai-forever/Real-ESRGAN\",\n",
        "            \"filename\": \"RealESRGAN_x2.pth\",\n",
        "            \"local_dir\": \"models/upscalers/\"\n",
        "        },\n",
        "        \"RealESRGAN x4\": {\n",
        "            \"repo_id\": \"ai-forever/Real-ESRGAN\",\n",
        "            \"filename\": \"RealESRGAN_x4.pth\",\n",
        "            \"local_dir\": \"models/upscalers/\"\n",
        "        },\n",
        "        \"Embedding (BadPrompt)\": {\n",
        "            \"repo_id\": \"philz1337x/embeddings\",\n",
        "            \"filename\": \"verybadimagenegative_v1.3.pt\",\n",
        "            \"local_dir\": \"models/embeddings\"\n",
        "        },\n",
        "        \"Embedding (Juggernaut Neg)\": {\n",
        "            \"repo_id\": \"philz1337x/embeddings\",\n",
        "            \"filename\": \"JuggernautNegative-neg.pt\",\n",
        "            \"local_dir\": \"models/embeddings\"\n",
        "        },\n",
        "        \"LoRA (SDXLrender)\": {\n",
        "            \"repo_id\": \"philz1337x/loras\",\n",
        "            \"filename\": \"SDXLrender_v2.0.safetensors\",\n",
        "            \"local_dir\": \"models/Lora\"\n",
        "        },\n",
        "        \"LoRA (More Details)\": {\n",
        "            \"repo_id\": \"philz1337x/loras\",\n",
        "            \"filename\": \"more_details.safetensors\",\n",
        "            \"local_dir\": \"models/Lora\"\n",
        "        },\n",
        "        \"ControlNet Tile\": {\n",
        "            \"repo_id\": \"lllyasviel/ControlNet-v1-1\",\n",
        "            \"filename\": \"control_v11f1e_sd15_tile.pth\",\n",
        "            \"local_dir\": \"models/ControlNet\"\n",
        "        },\n",
        "        \"VAE\": {\n",
        "            \"repo_id\": \"stabilityai/sd-vae-ft-mse-original\",\n",
        "            \"filename\": \"vae-ft-mse-840000-ema-pruned.safetensors\",\n",
        "            \"local_dir\": \"models/VAE\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    failed_downloads = []\n",
        "\n",
        "    for model_name, model_info in models.items():\n",
        "        file_path = os.path.join(model_info[\"local_dir\"], model_info[\"filename\"])\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"\\n  ‚Üí Downloading {model_name}...\")\n",
        "            try:\n",
        "                hf_hub_download(\n",
        "                    repo_id=model_info[\"repo_id\"],\n",
        "                    filename=model_info[\"filename\"],\n",
        "                    local_dir=model_info[\"local_dir\"]\n",
        "                )\n",
        "                if os.path.exists(file_path):\n",
        "                    size_mb = os.path.getsize(file_path) / (1024*1024)\n",
        "                    print(f\"    ‚úì Downloaded ({size_mb:.1f} MB)\")\n",
        "                else:\n",
        "                    print(f\"    ‚úó Download failed\")\n",
        "                    failed_downloads.append(model_name)\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚úó Error: {str(e)}\")\n",
        "                failed_downloads.append(model_name)\n",
        "        else:\n",
        "            size_mb = os.path.getsize(file_path) / (1024*1024)\n",
        "            print(f\"  ‚úì {model_name} already exists ({size_mb:.1f} MB)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VERIFYING INSTALLATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    all_files = [\n",
        "        'models/ControlNet/control_v11f1e_sd15_tile.pth',\n",
        "        'models/models/Stable-diffusion/juggernaut_reborn.safetensors',\n",
        "        'models/VAE/vae-ft-mse-840000-ema-pruned.safetensors',\n",
        "        'models/embeddings/verybadimagenegative_v1.3.pt',\n",
        "        'models/embeddings/JuggernautNegative-neg.pt',\n",
        "        'models/Lora/SDXLrender_v2.0.safetensors',\n",
        "        'models/Lora/more_details.safetensors',\n",
        "        'models/upscalers/RealESRGAN_x2.pth',\n",
        "        'models/upscalers/RealESRGAN_x4.pth'\n",
        "    ]\n",
        "\n",
        "    missing_files = []\n",
        "    total_size = 0\n",
        "\n",
        "    print(\"\\nInstalled files:\")\n",
        "    for file in all_files:\n",
        "        if os.path.exists(file):\n",
        "            size_mb = os.path.getsize(file) / (1024*1024)\n",
        "            total_size += size_mb\n",
        "            print(f\"  ‚úì {os.path.basename(file)} ({size_mb:.1f} MB)\")\n",
        "        else:\n",
        "            print(f\"  ‚úó {os.path.basename(file)} - MISSING\")\n",
        "            missing_files.append(file)\n",
        "\n",
        "    print(f\"\\nTotal size: {total_size:.1f} MB\")\n",
        "\n",
        "    if missing_files or failed_downloads:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"‚ö†Ô∏è  INSTALLATION INCOMPLETE\")\n",
        "        print(\"=\" * 70)\n",
        "        if failed_downloads:\n",
        "            print(\"\\nFailed downloads:\")\n",
        "            for item in failed_downloads:\n",
        "                print(f\"  ‚Ä¢ {item}\")\n",
        "        if missing_files:\n",
        "            print(\"\\nMissing files:\")\n",
        "            for file in missing_files:\n",
        "                print(f\"  ‚Ä¢ {os.path.basename(file)}\")\n",
        "        print(\"\\n‚Üí Please run this cell again to retry downloading missing files.\")\n",
        "        print(\"=\" * 70)\n",
        "    else:\n",
        "        # –°–æ–∑–¥–∞–µ–º –º–∞—Ä–∫–µ—Ä —É—Å–ø–µ—à–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
        "        open('/content/installed_tile_upscaler', 'w').close()\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"‚úÖ INSTALLATION COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"\\nYou can now proceed to:\")\n",
        "        print(\"  ‚Üí Cell 2: Upload your image\")\n",
        "        print(\"  ‚Üí Cell 3: Configure settings\")\n",
        "        print(\"  ‚Üí Cell 4: Process image\")\n",
        "        print(\"  ‚Üí Cell 5: Visualize results\")\n",
        "        print(\"  ‚Üí Cell 6: Download result\")\n",
        "        print(\"=\" * 70)\n",
        "else:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚úÖ ALREADY INSTALLED\")\n",
        "    print(\"=\" * 70)\n",
        "    os.chdir('/content/Tile-Upscaler')\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö\n",
        "    print(\"\\nInstalled models:\")\n",
        "    all_files = [\n",
        "        'models/ControlNet/control_v11f1e_sd15_tile.pth',\n",
        "        'models/models/Stable-diffusion/juggernaut_reborn.safetensors',\n",
        "        'models/VAE/vae-ft-mse-840000-ema-pruned.safetensors',\n",
        "        'models/embeddings/verybadimagenegative_v1.3.pt',\n",
        "        'models/embeddings/JuggernautNegative-neg.pt',\n",
        "        'models/Lora/SDXLrender_v2.0.safetensors',\n",
        "        'models/Lora/more_details.safetensors',\n",
        "        'models/upscalers/RealESRGAN_x2.pth',\n",
        "        'models/upscalers/RealESRGAN_x4.pth'\n",
        "    ]\n",
        "\n",
        "    total_size = 0\n",
        "    for file in all_files:\n",
        "        if os.path.exists(file):\n",
        "            size_mb = os.path.getsize(file) / (1024*1024)\n",
        "            total_size += size_mb\n",
        "            print(f\"  ‚úì {os.path.basename(file)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    print(f\"\\nTotal size: {total_size:.1f} MB\")\n",
        "    print(\"\\n‚Üí To reinstall: !rm /content/installed_tile_upscaler\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏\n",
        "if '/content/Real-ESRGAN' not in sys.path:\n",
        "    sys.path.insert(0, '/content/Real-ESRGAN')\n",
        "\n",
        "print(\"\\n‚úì Environment ready\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Upload Image** { display-mode: \"form\" }\n",
        "\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "print(\"üìÅ Please select an image to upload...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    input_image = Image.open(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "    input_image.save('/content/input_image.png')\n",
        "\n",
        "    print(f\"‚úÖ Image uploaded successfully: {filename}\")\n",
        "    print(f\"   Size: {input_image.size}\")\n",
        "\n",
        "    #display(input_image)\n",
        "else:\n",
        "    print(\"‚ùå No image uploaded!\")"
      ],
      "metadata": {
        "id": "TZbG_Nc1Csyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Configuration** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### Output Settings\n",
        "resolution = 2048 #@param {type:\"slider\", min:512, max:4096, step:128}\n",
        "\n",
        "#@markdown ### Generation Settings\n",
        "num_inference_steps = 50 #@param {type:\"slider\", min:10, max:100, step:5}\n",
        "strength = 0.3 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "guidance_scale = 3 #@param {type:\"slider\", min:1, max:20, step:0.5}\n",
        "controlnet_strength = 0.2 #@param {type:\"slider\", min:0.0, max:2.0, step:0.1}\n",
        "\n",
        "#@markdown ### Effects\n",
        "hdr_effect = 0 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "face_enhance = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Scheduler\n",
        "scheduler = \"DPM++ 3M SDE Karras\" #@param [\"DDIM\", \"DPM++ 3M SDE Karras\", \"DPM++ 3M Karras\"]\n",
        "\n",
        "#@markdown ### Prompt (Optional)\n",
        "custom_prompt = \"photo\" #@param {type:\"string\"}\n",
        "custom_negative_prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"‚öôÔ∏è Configuration saved!\")\n",
        "print(f\"Resolution: {resolution}\")\n",
        "print(f\"Steps: {num_inference_steps}\")\n",
        "print(f\"Strength: {strength}\")\n",
        "print(f\"Guidance Scale: {guidance_scale}\")\n",
        "print(f\"ControlNet Strength: {controlnet_strength}\")\n",
        "print(f\"HDR Effect: {hdr_effect}\")\n",
        "print(f\"Face Enhancement: {'Enabled' if face_enhance else 'Disabled'}\")\n",
        "print(f\"Scheduler: {scheduler}\")"
      ],
      "metadata": {
        "id": "0RyK9UWgCx-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Run** { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import warnings\n",
        "from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, DDIMScheduler, DPMSolverMultistepScheduler\n",
        "from diffusers.models import AutoencoderKL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from realesrgan import RealESRGANer\n",
        "import random\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "os.chdir('/content/Tile-Upscaler')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_scheduler(scheduler_name, config):\n",
        "    if scheduler_name == \"DDIM\":\n",
        "        return DDIMScheduler.from_config(config)\n",
        "    elif scheduler_name == \"DPM++ 3M SDE Karras\":\n",
        "        return DPMSolverMultistepScheduler.from_config(config, algorithm_type=\"sde-dpmsolver++\", use_karras_sigmas=True)\n",
        "    elif scheduler_name == \"DPM++ 3M Karras\":\n",
        "        return DPMSolverMultistepScheduler.from_config(config, algorithm_type=\"dpmsolver++\", use_karras_sigmas=True)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheduler: {scheduler_name}\")\n",
        "\n",
        "class LazyRealESRGAN:\n",
        "    def __init__(self, device, scale):\n",
        "        self.device = device\n",
        "        self.scale = scale\n",
        "        self.model = None\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model is None:\n",
        "            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=self.scale)\n",
        "            model_path = f'models/upscalers/RealESRGAN_x{self.scale}.pth'\n",
        "\n",
        "            self.model = RealESRGANer(\n",
        "                scale=self.scale,\n",
        "                model_path=model_path,\n",
        "                model=model,\n",
        "                tile=512,\n",
        "                tile_pad=10,\n",
        "                pre_pad=0,\n",
        "                half=True if 'cuda' in str(self.device) else False,\n",
        "                device=self.device\n",
        "            )\n",
        "\n",
        "    def predict(self, img):\n",
        "        self.load_model()\n",
        "        img_np = np.array(img)\n",
        "        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "        output, _ = self.model.enhance(img_bgr, outscale=self.scale)\n",
        "        output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
        "        return Image.fromarray(output_rgb)\n",
        "\n",
        "class LazyGFPGAN:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model is None:\n",
        "            from gfpgan import GFPGANer\n",
        "\n",
        "            model_path = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth'\n",
        "\n",
        "            self.model = GFPGANer(\n",
        "                model_path=model_path,\n",
        "                upscale=1,\n",
        "                arch='clean',\n",
        "                channel_multiplier=2,\n",
        "                bg_upsampler=None,\n",
        "                device=self.device\n",
        "            )\n",
        "\n",
        "    def enhance(self, img):\n",
        "        self.load_model()\n",
        "        img_np = np.array(img)\n",
        "        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "        _, _, output = self.model.enhance(img_bgr, has_aligned=False, only_center_face=False, paste_back=True)\n",
        "        output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
        "        return Image.fromarray(output_rgb)\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π\n",
        "if 'lazy_realesrgan_x2' not in globals():\n",
        "    lazy_realesrgan_x2 = LazyRealESRGAN(device, scale=2)\n",
        "    lazy_realesrgan_x4 = LazyRealESRGAN(device, scale=4)\n",
        "\n",
        "if 'lazy_gfpgan' not in globals():\n",
        "    lazy_gfpgan = LazyGFPGAN(device)\n",
        "\n",
        "if 'pipe' not in globals():\n",
        "    print(\"üîÑ Loading models...\")\n",
        "\n",
        "    # –ü–æ–¥–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ –æ—Ç diffusers\n",
        "    from diffusers.utils import logging as diffusers_logging\n",
        "    diffusers_logging.set_verbosity_error()\n",
        "\n",
        "    controlnet = ControlNetModel.from_single_file(\n",
        "        \"models/ControlNet/control_v11f1e_sd15_tile.pth\", torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    pipe = StableDiffusionControlNetImg2ImgPipeline.from_single_file(\n",
        "        \"models/models/Stable-diffusion/juggernaut_reborn.safetensors\",\n",
        "        controlnet=controlnet,\n",
        "        torch_dtype=torch.float16,\n",
        "        use_safetensors=True,\n",
        "        safety_checker=None\n",
        "    )\n",
        "\n",
        "    vae = AutoencoderKL.from_single_file(\n",
        "        \"models/VAE/vae-ft-mse-840000-ema-pruned.safetensors\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    pipe.vae = vae\n",
        "\n",
        "    pipe.load_textual_inversion(\"models/embeddings/verybadimagenegative_v1.3.pt\")\n",
        "    pipe.load_textual_inversion(\"models/embeddings/JuggernautNegative-neg.pt\")\n",
        "    pipe.load_lora_weights(\"models/Lora/SDXLrender_v2.0.safetensors\")\n",
        "    pipe.fuse_lora(lora_scale=0.5)\n",
        "    pipe.load_lora_weights(\"models/Lora/more_details.safetensors\")\n",
        "    pipe.fuse_lora(lora_scale=1.)\n",
        "\n",
        "    pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.3, b2=1.4)\n",
        "    pipe.to(device)\n",
        "\n",
        "    # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è pipeline\n",
        "    pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "    print(\"‚úÖ Models loaded!\")\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π scheduler\n",
        "pipe.scheduler = get_scheduler(scheduler, pipe.scheduler.config)\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "def progressive_upscale(input_image, target_resolution, steps=3):\n",
        "    current_image = input_image.convert(\"RGB\")\n",
        "    current_size = max(current_image.size)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        if current_size >= target_resolution:\n",
        "            break\n",
        "\n",
        "        scale_factor = min(2, target_resolution / current_size)\n",
        "        new_size = (int(current_image.width * scale_factor), int(current_image.height * scale_factor))\n",
        "\n",
        "        if scale_factor <= 1.5:\n",
        "            current_image = current_image.resize(new_size, Image.LANCZOS)\n",
        "        else:\n",
        "            current_image = lazy_realesrgan_x2.predict(current_image)\n",
        "\n",
        "        current_size = max(current_image.size)\n",
        "\n",
        "    if current_size != target_resolution:\n",
        "        aspect_ratio = current_image.width / current_image.height\n",
        "        if current_image.width > current_image.height:\n",
        "            new_size = (target_resolution, int(target_resolution / aspect_ratio))\n",
        "        else:\n",
        "            new_size = (int(target_resolution * aspect_ratio), target_resolution)\n",
        "        current_image = current_image.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    return current_image\n",
        "\n",
        "def create_hdr_effect(original_image, hdr):\n",
        "    if hdr == 0:\n",
        "        return original_image\n",
        "    cv_original = cv2.cvtColor(np.array(original_image), cv2.COLOR_RGB2BGR)\n",
        "    factors = [1.0 - 0.9 * hdr, 1.0 - 0.7 * hdr, 1.0 - 0.45 * hdr,\n",
        "              1.0 - 0.25 * hdr, 1.0, 1.0 + 0.2 * hdr,\n",
        "              1.0 + 0.4 * hdr, 1.0 + 0.6 * hdr, 1.0 + 0.8 * hdr]\n",
        "    images = [cv2.convertScaleAbs(cv_original, alpha=factor) for factor in factors]\n",
        "    merge_mertens = cv2.createMergeMertens()\n",
        "    hdr_image = merge_mertens.process(images)\n",
        "    hdr_image_8bit = np.clip(hdr_image * 255, 0, 255).astype('uint8')\n",
        "    return Image.fromarray(cv2.cvtColor(hdr_image_8bit, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "def prepare_image(input_image, resolution, hdr):\n",
        "    upscaled_image = progressive_upscale(input_image, resolution)\n",
        "    return create_hdr_effect(upscaled_image, hdr)\n",
        "\n",
        "def create_gaussian_weight(tile_size, sigma=0.3):\n",
        "    x = np.linspace(-1, 1, tile_size)\n",
        "    y = np.linspace(-1, 1, tile_size)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    gaussian_weight = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
        "    return gaussian_weight\n",
        "\n",
        "def adaptive_tile_size(image_size, base_tile_size=512, max_tile_size=1024):\n",
        "    w, h = image_size\n",
        "    aspect_ratio = w / h\n",
        "    if aspect_ratio > 1:\n",
        "        tile_w = min(w, max_tile_size)\n",
        "        tile_h = min(int(tile_w / aspect_ratio), max_tile_size)\n",
        "    else:\n",
        "        tile_h = min(h, max_tile_size)\n",
        "        tile_w = min(int(tile_h * aspect_ratio), max_tile_size)\n",
        "    return max(tile_w, base_tile_size), max(tile_h, base_tile_size)\n",
        "\n",
        "def process_tile(tile, num_inference_steps, strength, guidance_scale, controlnet_strength):\n",
        "    prompt = custom_prompt if custom_prompt else \"masterpiece, best quality, highres\"\n",
        "    negative_prompt = custom_negative_prompt if custom_negative_prompt else \"low quality, normal quality, ugly, blurry, blur, lowres, bad anatomy, bad hands, cropped, worst quality, verybadimagenegative_v1.3, JuggernautNegative-neg\"\n",
        "\n",
        "    options = {\n",
        "        \"prompt\": prompt,\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"image\": tile,\n",
        "        \"control_image\": tile,\n",
        "        \"num_inference_steps\": num_inference_steps,\n",
        "        \"strength\": strength,\n",
        "        \"guidance_scale\": guidance_scale,\n",
        "        \"controlnet_conditioning_scale\": float(controlnet_strength),\n",
        "        \"generator\": torch.Generator(device=device).manual_seed(random.randint(0, 2147483647)),\n",
        "    }\n",
        "\n",
        "    return np.array(pipe(**options).images[0])\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "input_image = Image.open('/content/input_image.png')\n",
        "\n",
        "print(\"üé® Processing image...\")\n",
        "start_time = time.time()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "condition_image = prepare_image(input_image, resolution, hdr_effect)\n",
        "W, H = condition_image.size\n",
        "\n",
        "# Adaptive tiling\n",
        "tile_width, tile_height = adaptive_tile_size((W, H))\n",
        "\n",
        "overlap = min(64, tile_width // 8, tile_height // 8)\n",
        "num_tiles_x = math.ceil((W - overlap) / (tile_width - overlap))\n",
        "num_tiles_y = math.ceil((H - overlap) / (tile_height - overlap))\n",
        "\n",
        "result = np.zeros((H, W, 3), dtype=np.float32)\n",
        "weight_sum = np.zeros((H, W, 1), dtype=np.float32)\n",
        "\n",
        "gaussian_weight = create_gaussian_weight(max(tile_width, tile_height))\n",
        "\n",
        "total_tiles = num_tiles_x * num_tiles_y\n",
        "\n",
        "# –û–¥–∏–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –≤—Å–µ—Ö —Ç–∞–π–ª–æ–≤\n",
        "with tqdm(total=total_tiles, desc=\"Processing tiles\", unit=\"tile\") as pbar:\n",
        "    for i in range(num_tiles_y):\n",
        "        for j in range(num_tiles_x):\n",
        "            left = j * (tile_width - overlap)\n",
        "            top = i * (tile_height - overlap)\n",
        "            right = min(left + tile_width, W)\n",
        "            bottom = min(top + tile_height, H)\n",
        "\n",
        "            current_tile_size = (bottom - top, right - left)\n",
        "\n",
        "            tile = condition_image.crop((left, top, right, bottom))\n",
        "            tile = tile.resize((tile_width, tile_height))\n",
        "\n",
        "            result_tile = process_tile(tile, num_inference_steps, strength, guidance_scale, controlnet_strength)\n",
        "\n",
        "            if current_tile_size != (tile_width, tile_height):\n",
        "                result_tile = cv2.resize(result_tile, current_tile_size[::-1])\n",
        "                tile_weight = cv2.resize(gaussian_weight, current_tile_size[::-1])\n",
        "            else:\n",
        "                tile_weight = gaussian_weight[:current_tile_size[0], :current_tile_size[1]]\n",
        "\n",
        "            result[top:bottom, left:right] += result_tile * tile_weight[:, :, np.newaxis]\n",
        "            weight_sum[top:bottom, left:right] += tile_weight[:, :, np.newaxis]\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "final_result = (result / weight_sum).astype(np.uint8)\n",
        "output_image = Image.fromarray(final_result)\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º GFPGAN –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–∏—Ü –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ\n",
        "if face_enhance:\n",
        "    print(\"üë§ Enhancing faces...\")\n",
        "    output_image = lazy_gfpgan.enhance(output_image)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "output_image.save('/content/output_image.png')\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"‚úÖ Done in {end_time - start_time:.1f}s | Size: {output_image.size}\")"
      ],
      "metadata": {
        "id": "-szDmqf_C1XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Visualize** { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "from IPython.display import HTML, display\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "#@markdown ### Visualization Method\n",
        "visualization_method = \"Slider\" #@param [\"Side-by-Side\", \"Slider\"]\n",
        "\n",
        "def resize_image_maintain_aspect(image, max_width, target_height=None):\n",
        "    width, height = image.size\n",
        "    if width > max_width:\n",
        "        new_height = int(height * max_width / width)\n",
        "        image = image.resize((max_width, new_height), PIL.Image.LANCZOS)\n",
        "    if target_height is not None and image.size[1] != target_height:\n",
        "        new_width = int(image.size[0] * target_height / image.size[1])\n",
        "        image = image.resize((new_width, target_height), PIL.Image.LANCZOS)\n",
        "    return image\n",
        "\n",
        "def image_to_base64(image):\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"PNG\")\n",
        "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "if not os.path.exists('/content/input_image.png') or not os.path.exists('/content/output_image.png'):\n",
        "    print(\"‚ùå Error: Images not found. Please run the processing step first.\")\n",
        "else:\n",
        "    image_original = PIL.Image.open('/content/input_image.png')\n",
        "    image_restore = PIL.Image.open('/content/output_image.png')\n",
        "\n",
        "    if visualization_method == \"Side-by-Side\":\n",
        "        max_width = 500\n",
        "        image_original = resize_image_maintain_aspect(image_original, max_width)\n",
        "        image_restore = resize_image_maintain_aspect(image_restore, max_width)\n",
        "        target_height = min(image_original.size[1], image_restore.size[1])\n",
        "        image_original = resize_image_maintain_aspect(image_original, max_width, target_height)\n",
        "        image_restore = resize_image_maintain_aspect(image_restore, max_width, target_height)\n",
        "\n",
        "        combined_width = image_original.size[0] + image_restore.size[0]\n",
        "        combined_image = PIL.Image.new('RGB', (combined_width, target_height))\n",
        "        combined_image.paste(image_original, (0, 0))\n",
        "        combined_image.paste(image_restore, (image_original.size[0], 0))\n",
        "        display(combined_image)\n",
        "\n",
        "    else:  # Slider\n",
        "        max_width = min(image_restore.size[0], 1000)\n",
        "        image_restore = resize_image_maintain_aspect(image_restore, max_width)\n",
        "        target_height = image_restore.size[1]\n",
        "        image_original = resize_image_maintain_aspect(image_original, max_width, target_height)\n",
        "\n",
        "        if image_original.mode != 'RGB':\n",
        "            image_original = image_original.convert('RGB')\n",
        "        if image_restore.mode != 'RGB':\n",
        "            image_restore = image_restore.convert('RGB')\n",
        "\n",
        "        original_base64 = image_to_base64(image_original)\n",
        "        restore_base64 = image_to_base64(image_restore)\n",
        "\n",
        "        html_code = f\"\"\"\n",
        "        <div style=\"position: relative; width: {image_restore.size[0]}px; height: {image_restore.size[1]}px; margin: 20px auto;\">\n",
        "            <div style=\"position: relative; width: 100%; height: 100%; overflow: hidden;\">\n",
        "                <img src=\"data:image/png;base64,{original_base64}\" style=\"position: absolute; width: 100%; height: 100%;\">\n",
        "                <div style=\"position: absolute; width: 100%; height: 100%; overflow: hidden; clip-path: inset(0 0 0 50%);\">\n",
        "                    <img src=\"data:image/png;base64,{restore_base64}\" style=\"position: absolute; width: 100%; height: 100%;\">\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"slider\" style=\"position: absolute; top: 0; bottom: 0; width: 4px; background: white; cursor: ew-resize; left: 50%; box-shadow: 0 0 5px rgba(0,0,0,0.5);\">\n",
        "                <div style=\"position: absolute; top: 50%; transform: translateY(-50%); width: 20px; height: 20px; background: white; border-radius: 50%; left: -8px;\"></div>\n",
        "            </div>\n",
        "        </div>\n",
        "        <script>\n",
        "            (function() {{\n",
        "                const slider = document.querySelector('.slider');\n",
        "                if (!slider) return;\n",
        "\n",
        "                let isDragging = false;\n",
        "                const container = slider.parentElement.querySelector('div:nth-child(1)');\n",
        "                const clipDiv = container.querySelector('div');\n",
        "\n",
        "                slider.addEventListener('mousedown', (e) => {{\n",
        "                    isDragging = true;\n",
        "                    e.preventDefault();\n",
        "                }});\n",
        "\n",
        "                document.addEventListener('mouseup', () => {{\n",
        "                    isDragging = false;\n",
        "                }});\n",
        "\n",
        "                document.addEventListener('mousemove', (e) => {{\n",
        "                    if (!isDragging) return;\n",
        "                    const rect = container.getBoundingClientRect();\n",
        "                    let x = e.clientX - rect.left;\n",
        "                    if (x < 0) x = 0;\n",
        "                    if (x > rect.width) x = rect.width;\n",
        "                    const percentage = (x / rect.width) * 100;\n",
        "                    slider.style.left = percentage + '%';\n",
        "                    clipDiv.style.clipPath = `inset(0 0 0 ${{percentage}}%)`;\n",
        "                }});\n",
        "\n",
        "                slider.addEventListener('touchstart', (e) => {{\n",
        "                    isDragging = true;\n",
        "                    e.preventDefault();\n",
        "                }});\n",
        "\n",
        "                document.addEventListener('touchend', () => {{\n",
        "                    isDragging = false;\n",
        "                }});\n",
        "\n",
        "                document.addEventListener('touchmove', (e) => {{\n",
        "                    if (!isDragging) return;\n",
        "                    const rect = container.getBoundingClientRect();\n",
        "                    let x = e.touches[0].clientX - rect.left;\n",
        "                    if (x < 0) x = 0;\n",
        "                    if (x > rect.width) x = rect.width;\n",
        "                    const percentage = (x / rect.width) * 100;\n",
        "                    slider.style.left = percentage + '%';\n",
        "                    clipDiv.style.clipPath = `inset(0 0 0 ${{percentage}}%)`;\n",
        "                }});\n",
        "            }})();\n",
        "        </script>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(html_code))"
      ],
      "metadata": {
        "id": "rTKy92zzC5Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Download** { display-mode: \"form\" }\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if os.path.exists('/content/output_image.png'):\n",
        "    print(\"üíæ Downloading upscaled image...\")\n",
        "    files.download('/content/output_image.png')\n",
        "    print(\"‚úÖ Download started!\")\n",
        "else:\n",
        "    print(\"‚ùå No output image found! Please run the processing step first.\")"
      ],
      "metadata": {
        "id": "obl8ZcZOC8p4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}